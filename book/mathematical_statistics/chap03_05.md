---
title: Chapter3-05. statistical hypothesis
book_title: Mathematical statistics
book_chapter: 3
book_section: 5
---

## 3.5 Statistical hypothesis


## 3.5.3. Formulation of hypothesis testing

### Definition testing function
* $(\mathcal{X}, \mathcal{A})$,
    * measurable space
* $\Theta \subseteq \mathbb{R}^{p}$,
    * parameters
* $$\{P\}_{\theta \in \Theta}$$,
    * probability measure over $(\mathcal{X},\mathcal{A})$,

Subset $\Theta_{H} \subseteq \Theta$ is said to be hypothesis.
If $\Theta_{H}$

<div class="end-of-statement" style="text-align: right">■</div>

### Example 3.25
* $n \in \mathbb{N}$,
* $\Theta := [0, 1]$,
* $\mathcal{X} := \{0, 1, \ldots, n\}$,
* $\mu: \mathcal{X} \rightarrow [0, 1]$
    * countable measure
* $P_{\theta}:= B(n, \theta)$
    * binominal distribution
* $\theta_{i} \in \Theta$,
    * $0 < \theta_{0} < \theta_{1} < 1$,

$$
\begin{eqnarray}
    p_{\theta}(x)
    & := &
        \frac{d P_{\theta}}{d \mu}(x)
    \nonumber
    \\
    & = &
        \left(
            \begin{array}{c}
                n \\
                x
            \end{array}
        \right)
        \theta^{x}
        (1 - \theta)^{n - x}
    \nonumber
\end{eqnarray}
$$

We consinder hypothesis test:

$$
    \Theta_{H}
    :=
    \{\theta_{0}\},
    \
    \Theta_{K}
    :=
    \{\theta_{0}\}
    .
$$

We have

$$
    \log
        \frac{
            p_{\theta_{0}}(x)
        }{
            p_{\theta_{1}}(x)
        }
    =
    ax
    +
    n \log
        \frac{
            1 - \theta_{1}
        }{
            1 - \theta_{0}
        }
$$

where

$$
    a
    :=
    \log
        \frac{
            \theta_{1}/(1 - \theta_{1})
        }{
            \theta_{0}/(1 - \theta_{0})
        }
    >
    0
    .
$$

<div class="end-of-statement" style="text-align: right">■</div>

### 3.5.5 Monotone likehood ratio and composite hypothesis test
* $\Theta \subseteq \mathbb{R}$,
* $(\mathcal{X}, \mathcal{A})$,
    * measurable space
* $$\mathcal{P} := \{P_{\theta} \}_{\theta \in \Theta}$$,
    * family of probability distribution over $(\mathcal{X}, \mathcal{A})$
* $\mu: \mathcal{X} \rightarrow [0, \infty)$
    * $\sigma$ finite measure on $(\mathcal{X}, \mathcal{A})$

Assumptions

$$
    \forall \theta,
    \
    \mu
    \gg
    P_{\theta}
    .
$$

### Definition monotone likelihodd ratio
* $T: \mathcal{X} \rightarrow \mathbb{R}_{\ge 0}$
    * measurable function

$$
    \mathcal{X}_{\theta_{1}, \theta_{2}}
    :=
    \mathcal{X}
    \setminus
    \{x \mid p_{\theta_{1}}(x) = p_{\theta_{2}}(x) = 0\}
    \quad
    (\theta_{1}, \theta_{2} \in \Theta)
$$

$\mathcal{P}$ is said to be monotone likelihood ration with respect to $T$ if

$$
\begin{eqnarray}
    \forall \theta_{1}, \theta_{2} \in \Theta,
    \
    \theta_{1} < \theta_{2},
    \
    \exists H_{\theta_{1}, \theta_{2}}:T(\mathcal{X}) \rightarrow [0, \infty]
    \text{ s.t. }
    & &
        H \text{ is non-decreasing},
    \nonumber
    \\
    & &
        \forall x \in
        \mathcal{X}_{\theta_{1}, \theta_{2}}
        ,
        \
        \frac{p_{\theta_{2}}}{p_{\theta_{1}}}(x)
        =
        H_{\theta_{1}, \theta_{2}}(T(x))
    \nonumber
\end{eqnarray}
$$

We assume $c/0 = \infty$ for $c > 0$.

<div class="end-of-statement" style="text-align: right">■</div>

### Example 3.26
* $\Theta \subseteq \mathbb{R}$,
* $$\mathcal{P} := \{P_{\theta} \}_{\theta \in \Theta}$$,
    * one prameter exponential family

That is, there exist $g: \mathcal{X} \rightarrow \mathbb{R}_{\ge 0}$, $a: \Theta \rightarrow \mathbb{R}$, $\psi: \Theta \rightarrow \mathbb{R}$ such that

$$
    p_{\theta}(x)
    :=
    g(x)
    \exp
    \left(
        a(\theta) T(x)
        -
        \psi(\theta)
    \right)
    \
    (x \in \mathcal{X})
    .
$$

If $a$ is non-decreasing function, then $\mathcal{P}$ is monotone likelihood ration with respect to $T$.

<div class="end-of-statement" style="text-align: right">■</div>

### Theorem 3.27
* $\theta_{0} \in \Theta$,
    * given
* $$\mathcal{P} := \{P_{\theta}\}_{\theta \in \Theta}$$,
    * monotone likelihood ratio with respect to $T$
* $$\{\theta \mid \theta > \theta_{0}\} \neq \emptyset$$,
* $c \in \mathbb{R}$,
* $\gamma \in [0, 1]$,

(a) Let

$$
\begin{eqnarray}
    \phi_{0}(x)
    & := &
        \begin{cases}
            1
            &
                (T(x) > c)
            \\
            \gamma
            &
                (T(x) = c)
            \\
            0
            &
                (T(x) < c)
        \end{cases}
    \nonumber
    \\
    & = &
        1_{\{x \mid T(x) >c\}}(x)
        +
        \gamma
        1_{\{x \mid T(x) =c\}}(x)
        \label{chap03_03_22_test}
\end{eqnarray}
$$

If $$\mathrm{E}_{\theta_{0}} \left[ \phi_{0} \right] > 0$$, then $\phi_{0}$ is the most powerful test at level $$\alpha^{\prime} := \mathrm{E}_{\theta_{0}}[\phi_{0}]$$ for hypothesis test

$$
\begin{equation}
    \Theta_{H}
    :=
    \{\theta \mid \theta \le \theta_{0}\},
    \
    \Theta_{K}
    :=
    \{\theta \mid \theta > \theta_{0}\},
    \label{chap03_03_23_hypothesis_test}
\end{equation}
$$

(b) For $\alpha \in (0, 1)$,

$$
    \exists c \in \mathbb{R},
    \
    \gamma \in [0, 1],
    \
    \text{ s.t. }
    \
    \phi_{0}: \eqref{chap03_03_23_hypothesis_test}
    \text{the most powerful test at level } \alpha
$$

### proof
(a)

Let $$\Phi(\Theta_{H}, \Theta_{K}, \alpha^{\prime})$$ be a set of tests for $$\Theta_{H}$$ and $$\Theta_{K}$$ at level $\alpha^{\prime}$.
We need to show

$$
\begin{equation}
    \forall \phi \in \Phi(\Theta_{H}, \Theta_{K}, \alpha^{\prime}),
    \
    \theta_{1} \in \Theta_{K},
    \
    \mathrm{E}_{\theta_{1}}[\phi_{0}]
    \ge
    \mathrm{E}_{\theta_{1}}[\phi]
    \label{chap03_monotone_likehood_ratio_ump_test}
\end{equation}
$$

and

$$
\begin{equation}
    \sup_{\theta \in \Theta_{H}}
        \mathrm{E}_{\theta}
        \left[
            \phi_{0}
        \right]
    \le
    \alpha^{\prime}
    \label{chap03_monotone_likehood_ratio_level_alpha_test}
\end{equation}
    .
$$

We first show $$\eqref{chap03_monotone_likehood_ratio_ump_test}$$.
Let $\theta_{1} \in \Theta_{K}$ be fixed and

$$
    k
    :=
    \inf
    \left\{
        \frac{p_{\theta_{1}}(x)}{p_{\theta_{0}}(x)}
        \mid
        x \in \mathcal{X}_{\theta_{1}, \theta_{2}},
        \
        T(x) \ge c
    \right\}
    .
$$

Then $k \in \mathbb{R}_{\ge 0}$.
Indeed, 

$$
\begin{eqnarray}
    \mu(
        \{
            x \in \mathcal{X}_{\theta_{0}, \theta_{1}}
            \mid
            p_{\theta_{0}}(x)
            \neq
            0,
            \
            T(x) \ge c
        \}
    )
    & = &
        \mu(
            \{
                x \in \mathcal{X}_{\theta_{0}, \theta_{1}}
                \mid
                T(x) \ge c
            \}
        )
    \nonumber
    \\
    & > &
        0
\end{eqnarray}
$$

since

$$
    \int_{\mathcal{X}_{\theta_{0}, \theta_{1}}}
        1_{\{T \ge c\}}(x)
        p_{\theta_{0}}(x)
    \ \mu(dx)
    =
    P_{\theta_{0}}(T \ge c)
    \ge
    \mathrm{E}_{\theta_{0}}[\phi_{0}]
    >
    0
    .
$$

Morever $$p_{\theta_{1}} < \infty \ \mu \text{-a.e.}$$ by definition.
Therefore $k < \infty$.

Now we show that 

$$
\begin{eqnarray}
    x \in \mathcal{X}_{\theta_{0}, \theta_{1}},
    \
    p_{\theta_{1}}(x)
    >
    kp_{\theta_{0}}(x),
    & \Rightarrow &
        \phi_{0}(x) = 1
    \nonumber
    \\
    x \in \mathcal{X}_{\theta_{0}, \theta_{1}},
    \
    p_{\theta_{1}}(x)
    <
    kp_{\theta_{0}}(x),
    & \Rightarrow &
        \phi_{0}(x) = 0
        \label{chap03_03_24}
    .
\end{eqnarray}
$$

Let $$x \in \mathcal{X}_{\theta_{0}, \theta_{1}}$$ be fixed.
Suppose that $$p_{\theta_{1}}(x)/p_{\theta_{0}}(x) > k$$.
To show $\phi(x) = 1$, it is sufficient to see $T(x) > c$.
By definition of $k$, there exists $$x^{\prime} \in \mathcal{X}_{\theta_{0}, \theta_{1}}$$ such that

$$
    k
    \le
    \frac{p_{\theta_{1}}(x^{\prime})}{p_{\theta_{0}}(x^{\prime})}
    <
    \frac{p_{\theta_{1}}(x)}{p_{\theta_{0}}(x)},
    \
    T(x^{\prime})
    \ge
    c
    .
$$

If we assume $T(x) \le c$, by definiiton of monotone likelihood ratio,

$$
\begin{eqnarray}
    \frac{
        p_{\theta_{1}}(x)
    }{
        p_{\theta_{0}}(x)
    }
    & = &
        H_{\theta_{0}, \theta_{1}}(T(x))
    \nonumber
    \\
    & \le &
        H_{\theta_{0}, \theta_{1}}(c)
    \nonumber
    \\
    & \le &
        H(T(x^{\prime}))
    \nonumber
    \\
    & \le &
        \frac{
            p_{\theta_{1}}(x^{\prime})
        }{
            p_{\theta_{0}}(x^{\prime})
        }
    \nonumber
\end{eqnarray}
$$

This is contradiction so that $\phi(x) = 1$.
Suppose that $$p_{\theta_{1}}(x)/p_{\theta_{0}}(x) < k$$.
If $T(x) \ge c$, $k$ cannot be infimum.
Hence $T(x) < c$.

From theorem 3.2.6 and $$\eqref{chap03_03_24}$$, test $\phi_{0}$ is the most powerful test of hypothesis test $$\Theta_{H}^{\prime} := \{\theta_{0}\}$$ and $$\Theta_{K}^{\prime} := \{\theta_{1}\}$$ at level $$\alpha^{\prime} := \mathrm{E}_{\theta_{0}}[\phi_{0}]$$.

Let $\phi$ be test for $$\Theta_{H}$$ and $$\Theta_{K}$$ at level $$\alpha^{\prime}$$.
Then $\phi$ is also test for $$\Theta_{H}^{\prime}$$ and $$\Theta_{K}^{\prime}$$ at level $$\alpha^{\prime}$$.
Hence

$$
\begin{equation}
    \mathrm{E}_{\theta_{1}}
    \left[
        \phi_{0}
    \right]
    \ge
    \mathrm{E}_{\theta_{1}}
    \left[
        \phi
    \right]
    \label{chap03_03_26}
\end{equation}
    .
$$

$$\theta_{1}$$ is arbitrary fixed so that $$\eqref{chap03_03_26}$$ holds for all $$\theta_{1} \in \Theta_{K}$$.

Now we show that $$\eqref{chap03_monotone_likehood_ratio_level_alpha_test}$$.
It suffices to show that

$$
    \forall \theta_{2} < \theta_{0},
    \
    \mathrm{E}_{\theta_{2}}
    \left[
        \phi_{0}
    \right]
    \le
    \alpha^{\prime}
$$

With out loss of generality, $$\mathrm{E}_{\theta_{2}}[\phi_{0}] > 0$$.
Indeed, if $$\mathrm{E}_{\theta_{2}}[\phi_{0}] = 0$$, the equation always holds  since $$0 \le \alpha^{\prime}$$.
Let $\theta_{2} < \theta_{0}$ be fixed.
From discussion above, $$\phi_{0}$$ is the most powerful test at level $$\alpha^{\prime\prime} := \mathrm{E}_{\theta_{2}}[\phi_{0}]$$ for hypothesis $$\Theta_{H}^{\prime\prime} := \{\theta_{2}\}$$ and alternatives $$\Theta_{K}^{\prime\prime} := \{\theta_{0} \}$$ by substituting $$\theta_{2}$$ for $$\theta_{0}$$ and $$\theta_{0}$$ for $$\theta_{1}$$, respectively.
Since $$\alpha^{\prime\prime}$$ is one of tests at level $$\alpha^{\prime\prime}$$ for hypothesis $$\Theta_{H}^{\prime\prime}$$ and alternatives $$\Theta_{K}^{\prime\prime}$$, we have

$$
\begin{eqnarray}
    &  &
        \mathrm{E}_{\theta_{0}}[\alpha^{\prime\prime}]
        \le
        \mathrm{E}_{\theta_{0}}[\phi_{0}]
    \nonumber
    \\
    & \Leftrightarrow &
        \alpha^{\prime\prime}
        \le
        \mathrm{E}_{\theta_{0}}[\phi_{0}]
    \nonumber
    \\
    & \Leftrightarrow &
        \mathrm{E}_{\theta_{2}}[\phi_{0}]
        \le
        \mathrm{E}_{\theta_{0}}[\phi_{0}]
        =
        \alpha^{\prime}
    \nonumber
\end{eqnarray}
$$

(b)

Let

$$
    F(u)
    :=
    P_{\theta_{0}}(T \le u)
    .
$$

Then there exists $c \in \mathbb{R}$ such that

$$
    F(c-)
    \le
    1 - \alpha
    \le
    F(c)
    .
$$

Now, let

$$
    \gamma
    :=
    \begin{cases}
        1/2
        &
        F(c) - F(c-) = 0
        \\
        \frac{
            (\alpha - 1 + F(c))
        }{
            F(c) - F(c-)
        }
        &
        F(c) - F(c-) > 0
    \end{cases}
    .
$$

Then $\phi_{0}$ defined in $$\eqref{chap03_03_22_test}$$ is the most powerful test at level $$\alpha := \mathrm{E}_{\theta_{0}}[\phi_{0}]$$ for hypothesis $$\Theta_{H}$$ and alternative $$\Theta_{K}$$ by (a).

<div class="end-of-statement" style="text-align: right">■</div>

### Remark
In the proof of (b), $\gamma$ 

<div class="end-of-statement" style="text-align: right">■</div>

## 3.5.6 Generalized Neyman Peason's lemma

### Theorem 3.28 Generalized neyman pearson fundamental lemma
* $(\mathcal{X}, \mathcal{A})$,
    * measurable sp.
* $\mu: \Omega \rightarrow [0, \infty)$,
    * $\sigma$-finite measure over $(\mathcal{X}, \mathcal{A})$
* $$\Phi := \{\phi \mid \phi: \mathcal{X} \rightarrow [0, 1]: \text{ measurable function}\}$$,
* $$f_{1}, \ldots, f_{m}, g \in L^{1}(\mathcal{X}, \mathcal{A}, \mu)$$,

$$
    c := (c_{1}, \ldots, c_{m})
    \in
    \mathbb{R}^{m},
    \
    \Phi_{c}
    :=
    \left\{
        \phi \in \Phi
        \mid
        \int_{\mathcal{X}}
            \phi(x) f_{i}(x)
        \ \mu(dx)
        =
        c_{i},
        (i = 1, \ldots, m)
    \right\}
    \neq
    \emptyset
    .
$$

Then

(a) Let $$\phi_{0} \in \Phi_{c}$$.
If there eixist $$k_{1}, \ldots, k_{m} \in \mathbb{R}$$ such that

$$
\begin{equation}
    \phi_{0}(x)
    =
    \begin{cases}
        1
        &
        (g(x) > \sum_{i=1}^{m}k_{i}f_{i}(x))
        \\
        0
        &
        (g(x) < \sum_{i=1}^{m}k_{i}f_{i}(x))
    \end{cases}
    \
    \mu \text{-a.e.}
    \label{chap03_03_27_test}
\end{equation}
    ,
$$

then

$$
    \int_{\mathcal{X}}
        \phi_{0}(x) g(x)
    \ \mu(dx)
    =
    \sup
    \left\{
        \int_{\mathcal{X}}
            \phi(x) g(x)
        \ \mu(dx)
        \mid
        \phi \in \Phi_{c}
    \right\}
    .
$$

(b) Let $$\phi_{0} \in \Phi_{c}$$.
If there exists $$k_{1}, \ldots, k_{m} \in \mathbb{R}_{\ge 0}$$ such that $$\eqref{chap03_03_27_test}$$ is satisfied, then

$$
    \int_{\mathcal{X}}
        \phi_{0}(x) g(x)
    \ \mu(dx)
    =
    \sup
    \left\{
        \int_{\mathcal{X}}
            \phi(x) g(x)
        \ \mu(dx)
        \mid
        \phi \in \Phi,
        \
        \int_{\mathcal{X}}
            \phi(x) f_{i}(x)
        \ \mu(dx)
        \le
        c_{i}
        (i = 1, \ldots, m)
    \right\}
$$

### proof
(a)

Since $$\phi_{0} \in \Phi_{c}$$,

$$
\begin{eqnarray}
    \int_{\mathcal{X}}
        \phi_{0}(x) g(x)
    \ \mu(dx)
    -
    \sum_{i=1}^{m}
        k_{i}c_{i}
    & = &
        \int_{\mathcal{X}}
            \phi_{0}(x)
            g(x)
        \ \mu(dx)
                -
        \int_{\mathcal{X}}
                \sum_{j=1}
                    k_{i}f_{i}(x)
        \ \mu(dx)
    \nonumber
    \\
    & = &
        \int_{\mathcal{X}}
            \phi_{0}(x)
            \left(
                g(x)
                -
                \sum_{j=1}
                    k_{i}f_{i}(x)
            \right)
        \ \mu(dx)
    \nonumber
\end{eqnarray}
$$

On the other hand, for all $\phi \in \Phi_{c}$

$$
\begin{eqnarray}
    \int_{\mathcal{X}}
        \phi(x) g(x)
    \ \mu(dx)
    -
    \sum_{i=1}^{m}
        k_{i}c_{i}
    & = &
        \int_{\mathcal{X}}
            \phi(x)
                g(x)
        \ \mu(dx)
                -
        \int_{\mathcal{X}}
                \sum_{j=1}
                    k_{i}f_{i}(x)
        \ \mu(dx)
    \nonumber
    \\
    & = &
        \int_{\mathcal{X}}
            \phi(x)
            \left(
                g(x)
                -
                \sum_{j=1}
                    k_{i}f_{i}(x)
            \right)
        \ \mu(dx)
    .
    \nonumber
\end{eqnarray}
$$

Therefore,

$$
\begin{eqnarray}
    \forall \phi \in \Phi_{c},
    \
    \int_{\mathcal{X}}
        \phi_{0}(x) g(x)
        -
        \phi(x) g(x)
    \ \mu(dx)
    & = &
        \int_{\mathcal{X}}
            \phi_{0}(x) g(x)
            -
            \phi(x) g(x)
        \ \mu(dx)
        +
        \sum_{i=1}^{m}
            k_{i}c_{i}
        -
        \sum_{i=1}^{m}
            k_{i}c_{i}
    \nonumber
    \\
    & = &
        \int_{\mathcal{X}}
            (\phi_{0}(x) - \phi(x))
            \left(
                g(x)
                -
                \sum_{j=1}
                    k_{i}f_{i}(x)
            \right)
        \ \mu(dx)
    \nonumber
    \\
    & = &
        \int_{g(x) > \sum_{j=1}^{m}k_{i}f_{i}(x)}
            (1 - \phi(x))
            \left(
                g(x)
                -
                \sum_{j=1}
                    k_{i}f_{i}(x)
            \right)
        \ \mu(dx)
    \nonumber
    \\
    & \ge &
        0
    .
    \nonumber
\end{eqnarray}
$$

(b)

For simplicity, let

$$
    B
    :=
    \left\{
        \phi \in \Phi
        \mid
        \int_{\mathcal{X}}
            \phi(x)f_{i}(x)
        \ \mu(dx),
        \le
        c_{i}
        \
        (i = 1, \ldots, m)
    \right\}
    .
$$

Since $\phi_{0} \in \Phi_{c}$, we have $\phi \in B$.
For all $\phi \in B$,

$$
\begin{eqnarray}
    \int_{\mathcal{X}}
        \phi(x) g(x)
    \ \mu(dx)
    -
    \sum_{i=1}^{m}
        k_{i}c_{i}
    & \le &
        \int_{\mathcal{X}}
            \phi(x)
                g(x)
        \ \mu(dx)
                -
        \int_{\mathcal{X}}
            \sum_{j=1}
                k_{i}f_{i}(x)
        \ \mu(dx)
    \nonumber
    \\
    & = &
        \int_{\mathcal{X}}
            \phi
            \left(
                g(x)
                -
                \sum_{j=1}
                    k_{i}f_{i}(x)
            \right)
        \ \mu(dx)
    .
    \nonumber
\end{eqnarray}
$$

Therefore,

$$
\begin{eqnarray}
    \forall \phi \in B,
    \
    \int_{\mathcal{X}}
        \phi_{0}(x) g(x)
        -
        \phi(x) g(x)
    \ \mu(dx)
    & = &
        \int_{\mathcal{X}}
            \phi_{0}(x) g(x)
        \ \mu(dx)
        -
        \sum_{i=1}^{m}
            k_{i}c_{i}
        -
        \left(
            \int_{\mathcal{X}}
                \phi(x) g(x)
            \ \mu(dx)
            -
            \sum_{i=1}^{m}
                k_{i}c_{i}
        \right)
    \nonumber
    \\
    & \ge &
        \int_{\mathcal{X}}
            (\phi_{0}(x) - \phi(x))
            \left(
                g(x)
                -
                \sum_{j=1}
                    k_{i}f_{i}(x)
            \right)
        \ \mu(dx)
    \nonumber
    \\
    & = &
        \int_{g(x) > \sum_{j=1}^{m}k_{i}f_{i}(x)}
            (1 - \phi(x))
            \left(
                g(x)
                -
                \sum_{j=1} k_{i}f_{i}(x)
            \right)
        \ \mu(dx)
    \nonumber
    \\
    & \ge &
        0
    .
    \nonumber
\end{eqnarray}
$$

<div class="end-of-statement" style="text-align: right">■</div>

### Remark

<div class="end-of-statement" style="text-align: right">■</div>

## 3.5.7 unbiased test


### Definition 3.29 Unbiased test
* $(\mathcal{X}, \mathcal{A})$,
    * measurable sp.
* $\Theta = \Theta_{0} \sqcup \Theta_{1}$,
    * parameter sp.
    * $\Theta_{0} \neq \emptyset$,
        * Hypothesis
    * $\Theta_{1} \neq \emptyset$,
        * Alternatives
* $\alpha \in [0, 1]$,
* $\phi: \mathcal{X} \rightarrow [0, 1]$
    * test at level$\alpha$

$\phi$ is said to be unibiased if

$$
    \forall \theta \in \Theta_{1},
    \
    \mathrm{E}_{\theta}
    \left[
        \phi
    \right]
    \ge
    \alpha
    .
$$

That is, power of the test $\phi$ is uniformly higher or equal to power of a trivial test $\phi^{\prime} \equiv \alpha$ at level $\alpha$.

We denote by $\phi_{\alpha}^{\mu}$ a set of all unibiased test at level $\alpha$.

<div class="end-of-statement" style="text-align: right">■</div>

### Remark
A trival test $\phi^{\prime} \equiv \alpha$ at level $\alpha$ is interpreted as the test is accepted at random with the probability $\alpha$.
An unbiased test at level $\alpha$ means that the unbiased test is not worse than at random.

<div class="end-of-statement" style="text-align: right">■</div>

### Definition 3.30
* $\Theta^{\prime} \subset \Theta$,
* $\phi$
    * test

$\phi$ is said to be similar to $\Theta^{\prime}$ if

$$
    \exists c \in \mathbb{R}
    \text{ s.t. }
    \forall \theta \in \Theta^{\prime},
    \
    c
    =
    \mathrm{E}_{\theta}
    \left[
        \phi
    \right]
    .
$$

<div class="end-of-statement" style="text-align: right">■</div>

### Proposition 3.31
* $\Theta$,
    * topological space
* $(\Theta_{i})^{b}$,
    * boundary of set $\Theta_{i}$,
* $\Theta^{\prime} := (\Theta_{0})^{b} \cap (\Theta_{1})^{b} \neq \emptyset$,
* $\phi: \mathcal{X} \rightarrow [0, 1]$,
    * unbiased test for Hypothesis $\Theta_{0}$ and alternative $\Theta_{1}$,
* $$\beta_{\phi}: \Theta \rightarrow [0, 1]$$,
    * continuous

Then $\phi$ is similar to $\Theta^{\prime}$.

### proof
Let $\theta \in \Theta^{\prime}$ be fixed.
Since $\beta_{\phi}$ is continuous,

$$
    \forall \epsilon > 0,
    \
    \exists \theta_{0} \in \Theta_{0},
    \
    \exists \theta_{1} \in \Theta_{1},
    \
    \text{ s.t. }
    \
    |
        \mathrm{E}_{\theta_{i}}
        \left[
            \phi
        \right]
        -
        \mathrm{E}_{\theta^{\prime}}
        \left[
            \phi
        \right]
    |
    \le
    \epsilon
    \
    (i = 0, 1)
$$

$\phi$ is unbiased,

$$
    \forall \theta \in \Theta_{1},
    \
    \mathrm{E}_{\theta}
    \left[
        \phi
    \right]
    \ge
    \alpha
$$

???

<div class="end-of-statement" style="text-align: right">■</div>

### Proposition 3.32
* $\Theta^{\prime} \neq \emptyset \subset \Theta$

$$
    \Phi_{\alpha}^{\prime}
    :=
    \{
        \phi \in \Phi
        \mid
        \mathrm{E}_{\theta}
        \left[
            \phi
        \right]
        =
        \alpha
        \
        (\theta \in \Theta^{\prime})
    \}
$$

* $\phi_{0} \in \Phi_{\alpha}^{\prime}$,

If $\phi_{0}$ satisfies

$$
\begin{eqnarray}
    \forall \theta \in \Theta_{0},
    \
    \forall \phi \in \Phi_{\alpha}^{\prime},
    \
    \mathrm{E}_{\theta}
    \left[
        \phi_{0}
    \right]
    & \le &
        \mathrm{E}_{\theta}
        \left[
            \phi
        \right]
    \nonumber
    \\
    \forall \theta \in \Theta_{1},
    \
    \forall \phi \in \Phi_{\alpha}^{\prime},
    \
    \mathrm{E}_{\theta}
    \left[
        \phi_{0}
    \right]
    & \ge &
        \mathrm{E}_{\theta}
        \left[
            \phi
        \right]
    \nonumber
\end{eqnarray}
$$

then, $\phi_{0}$ is a unbiased test at level $\alpha$ for hypothesis $\Theta_{0}$ and alternative $\Theta_{1}$.

### proof
Since $\phi \equiv \alpha \in \Phi_{\alpha}^{\prime}$ and assumptions of $\phi_{0}$, we obtain

$$
    \forall \theta \in \Theta_{0},
    \
    \mathrm{E}_{\theta}
    \left[
        \phi_{0}
    \right]
    \le
    \alpha
    .
$$

That is, $\phi_{0}$ is test at level $\alpha$.
Similarly, we obtain

$$
    \forall \theta \in \Theta_{1},
    \
    \mathrm{E}_{\theta}
    \left[
        \phi_{0}
    \right]
    \ge
    \alpha
    .
$$

Therefore $\phi_{0}$ is unbiased test.

<div class="QED" style="text-align: right">$\Box$</div>

### Remark
From both proposition, we can find an unbiased test by finding the best test only in  a set of similar tests.

<div class="end-of-statement" style="text-align: right">■</div>
