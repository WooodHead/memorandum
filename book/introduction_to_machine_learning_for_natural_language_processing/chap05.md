---
title: Chapter5. 系列ラベリング
book_title: Introduction to Machine Learning for Natural Language Processing
book_chapter: 5
---

## 5. 系列ラベリング
品詞ダグ付に用いる系列ラベリング。

* 5.1節
    * 系列ラベリングの性質
* 5.2節
    * 基本的な方法である隠れマルコフモデル
* 5.3節
    * 分類器を逐次的に用いて行う系列ラベリングについて
* 5.4節
    * 精度の良い条件付き確率場(CRF)
* 5.5節
    * チャンキングについて

## 5.1 準備
* 系列(sequence)
    * 要素が連なったもの、単語が連なった文
* 系列ラベリング(sequence labeling)
    * 単語の系列に品詞をつけるなど
    * 品詞タグ付けはsequence labelingの一種

* 品詞タグ付

## 5.2 隠れマルコフモデル
* 隠れマルコフモデル(Hidden Markov Model)

### 5.2.1 HMMの導入
簡単のため、

* $$X_{1:T} := (X_{1}, \ldots, X_{T})$$,
* $$Y_{1:T} := (Y_{1}, \ldots, Y_{T})$$,
* $$x_{1:T} := (x_{1}, \ldots, x_{T})$$,
* $$y_{1:T} := (y_{1}, \ldots, y_{T})$$,

とおく。
また、$Y_{t}$は隠れ変数でマルコフ過程である。
つまり、$Y_{t}$が与えられた時以下のマルコフ性が成り立つ。

$$
    p_{Y_{t+1} \mid Y_{1:t}}
    =
    p_{Y_{t+1} \mid Y_{t}}.
$$

$X_{t}$が$t$時点の隠れ変数$Y_{t}$によってのみ決まる変数である。
よって、ある関数$g$によって

$$
    X_{t}
    =
    g(t, Y_{t})
$$

となる。
このとき、$X_{t}$の分布は$Y_{t}$の関すとしてかける。

$$\{X_{t}\}_{t=1}^{T}$$のみを観測することができるという設定のもと$$Y_{T+1}$$を推定する。
推定の仕方は色々考えられるが、

$$
    y^{*}
    :=
    \argmax_{y_{1:T}}
        p_{X_{1:T}, Y_{1:T}}(x_{1:T}, y_{1:T})
$$

マルコフ性より、$X$と$Y$の条件付き確率密度関数は以下のようにかける。

$$
\begin{eqnarray}
    p_{X_{1:T}, Y_{1:T}}(x_{1:T}, y_{1:T})
    & = &
        p_{X_{T}, Y_{T} \mid X_{1:T-1}, Y_{1:T-1}}(x_{T}, y_{T} \mid x_{1:T-1}, y_{1:T-1})
            p_{X_{1:T-1}, Y_{1:T-1}}(x_{1:T-1}, y_{1:T-1})
    \nonumber
    \\
    & = &
        p_{X_{T}, Y_{T} \mid X_{T-1} Y_{T-1}}(x_{T}, y_{T} \mid x_{T-1}, y_{T-1})
            p_{X_{T-1}, Y_{T-1} \mid X_{T-2} Y_{T-2}}(x_{T-1}, y_{T-1} \mid x_{T-2}, y_{T-2})
    \nonumber
    \\
    & &
        \cdots
            p_{X_{2}, Y_{2} \mid X_{1}, Y_{1}}(x_{2}, y_{2} \mid x_{1}, y_{1})
            p_{X_{1}, Y_{1}}(x_{1}, y_{1})
    \nonumber
    \\
    & = &
        \prod_{t=1}^{T} p_{X_{t}, Y_{t} \mid X_{t-1}, Y_{t-1}}(x_{t}, y_{t} \mid x_{t-1}, y_{t-1})
            p_{X_{1}, Y_{1}}(x_{1}, y_{1})
\end{eqnarray}
$$

更に、

$$
\begin{eqnarray}
    p_{X_{t}, Y_{t} \mid X_{t-1}, Y_{t-1}}(x_{t}, y_{t} \mid x_{t-1}, y_{t-1})
    & = &
        p_{X_{t}, Y_{t} \mid Y_{t-1}}(x_{t}, y_{t} \mid y_{t-1})
    \nonumber
    \\
    & = &
        \frac{
            p_{X_{t}, Y_{t}, Y_{t-1}}(x_{t}, y_{t}, y_{t-1})
        }{
            p_{Y_{t-1}}(y_{t-1})
        }
    \nonumber
    \\
    & = &
        p_{X_{t} \mid Y_{t}, Y_{t-1}}(x_{t} \mid y_{t}, y_{t-1})
            \frac{
                p_{Y_{t}, Y_{t-1}}(y_{t}, y_{t-1})
            }{
                p_{Y_{t-1}}(y_{t-1})
            }
    \nonumber
    \\
    & = &
        p_{X_{t} \mid Y_{t}}(x_{t} \mid y_{t})
        p_{Y_{t} \mid Y_{t-1}}(y_{t} \mid y_{t-1})
\end{eqnarray}
$$

となってほしい。
最初の等号は、$X_{t-1}$が$Y_{t-1}$で決定されるから、条件付き確率において意味をなさないから？
最後の等号は、$Y_{t}$がマルコフなことによるから？
以上を認めれば以下のようにかける。

$$
\begin{eqnarray}
    p_{X_{1:T}, Y_{1:T}}(x_{1:T}, y_{1:T})
    & = &
        \prod_{t=1}^{T}
            \left(
                p_{X_{t} \mid Y_{t}}(x_{t} \mid y_{t})
                p_{Y_{t} \mid Y_{t-1}}(y_{t} \mid y_{t-1})
            \right)
            p_{X_{1}, Y_{1}}(x_{1}, y_{1})
\end{eqnarray}
$$

ここでは、潜在変数$Y_{t}$は文の$t$番目のラベル（品詞）で、$X_{t}$は文の$t$番目の単語である。

## 5.4 条件付き確率場
条件付き確率場は対数線形モデル。
対数線形でないといけない理由はなさそうだが？

## 5.4.1 条件付き確率場の導入
Conditional Random Fields でCRFとも言われる。

* $$X_{1:T} := (X_{1}, \ldots, X_{T})$$,
    * $T$個の単語の列
    * 一般には文
    * $T$が添字に付いているが、HMMのように必ずしも時系列性を考慮しているわけではないので、$T$次元の確率変数のベクトルと思っても良い
* $$Y_{1:T} := (Y_{1}, \ldots, Y_{T})$$,
    * $$\forall t = 1, \ldots, T, Y_{t}:\Omega \rightarrow \{1, \ldots, K\}$$と仮定する
        * 離散値に値をとることは重要
        * 各$t$について1から$K$は、特に一般性を失わないので仮定している(離散値なので最大値へ拡張すれば良い）
    * 単語に対応する品詞のラベル
    * 一般には文の各単語のラベル
    * $T$が添字に付いているが、HMMのように必ずしも時系列性を考慮しているわけではないので、$T$次元の確率変数のベクトルと思っても良い

$Y$のとる値の集合を以下のように定義しておく

データとして$$X_{1:T}$$, $$Y_{1:T}$$の独立同分布な確率変数の実現値が与えられているとする。
つまり、

* $$(X_{1:T}^{(i)})_{i} \quad (\forall i = 1, \ldots, N)$$,
    * $X$のi.i.d
* $$(Y_{1:T}^{(i)})_{i} \quad (\forall i = 1, \ldots, N)$$,
    * $Y$のi.i.d

$$
\begin{eqnarray}
    p_{X_{1:T} \mid Y_{1:T}}(x_{1:T}, \mid y_{1:T})
    & := &
        \frac{
            \exp
            \left(
                (w_{1:T})^{\mathrm{T}}
                \phi(x_{1:T}, y_{1:T})
            \right)
        }{
            Z(x_{1:T}, w_{1:T})
        },
    \\
    Z(x_{1:T}, w_{1:T})
    & := &
        \sum_{y_{1:T} \in \{1, \ldots, K\}^{T}}
            \exp
            \left(
                (w_{1:T})^{\mathrm{T}}
                \phi(x_{1:T}, y_{1:T})
            \right)
\end{eqnarray}
$$

とする。
ここで、$w_{1:T}$は素性に対する重みベクトルで、訓練データから学習し決定する。
$\phi(\cdot, \cdot)$は、4章導入した単語と単語ラベルから素性を対応づける適当な関数である。

$phi$を与え、$w_{1:T}$を学習すれば、CRFでの予測が可能となる。
CRFでは、同時確率を最大にするものを次の予測とする。
つまり、単語列$x_{1:T}$が与えられた時、そのラベル列を

$$
\begin{eqnarray}
    y_{1:T}^{*}
    & := &
        \argmax_{y_{1:T}}
            p_{X_{1:T} \mid Y_{1:T}}(x_{1:T}, \mid y_{1:T})
    \nonumber
    \\
    & = &
        \argmax_{y_{1:T}}
            \frac{
                \exp
                \left(
                    (w_{1:T})^{\mathrm{T}}
                    \phi(x_{1:T}, y_{1:T})
                \right)
            }{
                Z(x_{1:T}, w_{1:T})
            },
    \nonumber
    \\
    & = &
        \argmax_{y_{1:T}}
            (w_{1:T})^{\mathrm{T}}
                \phi(x_{1:T}, y_{1:T})
\end{eqnarray}
$$

として予測する。
$y_{1:T}$は$T$次元だから$y_{t}$の取る値の数$K$だけある。
上記の最大化は$O(K^{T})$で計算できるが、$K$はラベルの数、$T$は単語の列の長さとなって現実的でない。

上記の最大化の計算のために、仮定をおく。
仮定の置き方は色々考えられるが、ここでは$Y_{t}$がマルコフ性を持つとする。

TBD


### 5.4.2 条件付き確率場の学習
$w$の学習方法について触れる。
CRFは対数線形モデルなので、通常の一般化線形モデルの学習と同様の方法で学習できる。
つまり、

$$
    L(w)
    :=
    \sum_{i=1}^{N} 
        \log p_{Y_{1:T} \mid X_{1:T}}(y_{1:T} \mid x_{1:T})
        -
        \lambda \|w \|_{2}^{2}
$$

を$w$について最大化する問題を解く。
第二項は正則化項でここでは$l^{2}$正則化である。

TBD
