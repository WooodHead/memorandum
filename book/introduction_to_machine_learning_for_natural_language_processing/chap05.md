---
title: Chapter5. 系列ラベリング
book_title: Introduction to Machine Learning for Natural Language Processing
book_chapter: 5
---

## 5. 系列ラベリング
品詞ダグ付に用いる系列ラベリング。

* 5.1節
    * 系列ラベリングの性質
* 5.2節
    * 基本的な方法である隠れマルコフモデル
* 5.3節
    * 分類器を逐次的に用いて行う系列ラベリングについて
* 5.4節
    * 精度の良い条件付き確率場(CRF)
* 5.5節
    * チャンキングについて

## 5.1 準備
* 系列(sequence)
    * 要素が連なったもの、単語が連なった文
* 系列ラベリング(sequence labeling)
    * 単語の系列に品詞をつけるなど
    * 品詞タグ付けはsequence labelingの一種

* 品詞タグ付

## 5.2 隠れマルコフモデル
* 隠れマルコフモデル(Hidden Markov Model)

### 5.2.1 HMMの導入
簡単のため、

* $$X_{1:T} := (X_{1}, \ldots, X_{T})$$,
* $$Y_{1:T} := (Y_{1}, \ldots, Y_{T})$$,
* $$x_{1:T} := (x_{1}, \ldots, x_{T})$$,
* $$y_{1:T} := (y_{1}, \ldots, y_{T})$$,

とおく。
また、$Y_{t}$は隠れ変数でマルコフ過程である。
つまり、$Y_{t}$が与えられた時以下のマルコフ性が成り立つ。

$$
    p_{Y_{t+1} \mid Y_{1:t}}
    =
    p_{Y_{t+1} \mid Y_{t}}.
$$

$X_{t}$が$t$時点の隠れ変数$Y_{t}$によってのみ決まる変数である。
よって、ある関数$g$によって

$$
    X_{t}
    =
    g(t, Y_{t})
$$

となる。
このとき、$X_{t}$の分布は$Y_{t}$の関すとしてかける。

$$\{X_{t}\}_{t=1}^{T}$$のみを観測することができるという設定のもと$$Y_{T+1}$$を推定する。
推定の仕方は色々考えられるが、

$$
    y^{*}
    :=
    \argmax_{y_{1:T}}
        p_{X_{1:T}, Y_{1:T}}(x_{1:T}, y_{1:T})
$$

マルコフ性より、$X$と$Y$の条件付き確率密度関数は以下のようにかける。

$$
\begin{eqnarray}
    p_{X_{1:T}, Y_{1:T}}(x_{1:T}, y_{1:T})
    & = &
        p_{X_{T}, Y_{T} \mid X_{1:T-1}, Y_{1:T-1}}(x_{T}, y_{T} \mid x_{1:T-1}, y_{1:T-1})
            p_{X_{1:T-1}, Y_{1:T-1}}(x_{1:T-1}, y_{1:T-1})
    \nonumber
    \\
    & = &
        p_{X_{T}, Y_{T} \mid X_{T-1} Y_{T-1}}(x_{T}, y_{T} \mid x_{T-1}, y_{T-1})
            p_{X_{T-1}, Y_{T-1} \mid X_{T-2} Y_{T-2}}(x_{T-1}, y_{T-1} \mid x_{T-2}, y_{T-2})
    \nonumber
    \\
    & &
        \cdots
            p_{X_{2}, Y_{2} \mid X_{1}, Y_{1}}(x_{2}, y_{2} \mid x_{1}, y_{1})
            p_{X_{1}, Y_{1}}(x_{1}, y_{1})
    \nonumber
    \\
    & = &
        \prod_{t=1}^{T} p_{X_{t}, Y_{t} \mid X_{t-1}, Y_{t-1}}(x_{t}, y_{t} \mid x_{t-1}, y_{t-1})
            p_{X_{1}, Y_{1}}(x_{1}, y_{1})
\end{eqnarray}
$$

ここでは、潜在変数$Y_{t}$は文の$t$番目のラベル（品詞）で、$X_{t}$は文の$t$番目の単語である。
