---
title: Chapter2. 文章および単語の数学的表現
book_title: Introduction to Machine Learning for Natural Language Processing
book_chapter: 2
---

## 2. 文章および単語の数学的表現

* 2.1
    * タイプとトークンについて
* 2.2
    * nグラム
* 2.3
    * 文書や文のベクトル表現
* 2.4
    * 文書に対する前処理、言語処理、疎性について
* 2.5
    * 単語のベクトル表現
* 2.6
    * ベクトルではなく確率分布としての表現

文例

 Nurture or nature? Nurture passes nature.

## 2.1 タイプ、トークン
タイプとトークンの文書中の単語の種類。

* タイプ(type)、単語タイプ(word type)
    * word typeの方が良く使われる
    * 同じ単語は、1語と数える
    * 単語タイプ数を異なり語数ともいう
    * 文例だとタイプは4つ
* トークン(token)、単語トークン(word token)
    * 同じ単語でも、出現位置が異なれば別の語と数える
    * 単語トークン数を延べ語数ともいう
    * 文例だとトークンは6つ

## 2.2 nグラム
単語nグラムとは、 隣り合うn単語のこと。
順序は考慮しないので、隣り合うn単語の集合。
接続詞は抜く？
nに応じて特別な呼ばれ方をする。

* unigram
    * n=1のとき
* bigram
    * n=2
* trigram
    * n=3

文の最初と最後に意味を持たない記号を付加して、文の最初と最後を明示する場合がある。
例文はbigramでは

\{ {B,nurture} {nurture, nature}, {nurture, passess}, {passses, nature} \}

文字nグラムは、隣り合うn文字のこと。

* 英語の場合は空白も文字に含めることもできる
* 文字列の先頭と最後に特殊な記号をつけ加えることもある

## 2.3 文書、文のベクトル表現
* 素性(feature)、特徴(attribute)
    * ベクトルの各要素の意味
* 素性値(feature value)、特徴量(attribute value)
    * ベクトルの各要素の値

### 2.3.1 文書のベクトル表現
文書$d$からベクトル$x^{(d)}$を作る。

* word typeの出現頻度(bag-of-words)
    * 頻度ベクトル(frequency vector)
    * 語順、文の構造は失われる
    * bag-of-wordsの変種が良く使われる
* word typeの出現頻度(bag-of-n gram)
    * n gramの出現頻度
* word typeの出現有無
    * 出現したら1,しなければ0


* 単語辞書をあらかじめもっておき、出現判定をする
* 一般に疎なベクトルになる

### 2.3.2 文のベクトル表現
文書と一緒。

## 2.4 文書に対する前処理とデータスパースネス問題

### 2.4.1 文書に対する前処理

* ストップワード(stop word)
    * 文書の話題と関係ない語
    * `the`とかどんな文書にもでる`is`とか
    * 話題において重要でない語
    * 削除する
* ステミング(stemming)
    * `run`, `ran`, `running`などを派生語とみなす
    * 日本語の場合は活用形を同じ単語とみなすなど
    * 難しいので一般にルールベース
* 見出し語化(lemmatization)
    * `run`, `runs`などを基本形に戻すこと
    * 日本語だと動詞の派生語を戻す
* 品詞タグ付け
    * `fly`は動詞では飛ぶ、名詞ではハエ
    * 単語を品詞込で区別すること
* 語義の曖昧性解消(word sense disambiguation)
    * 品詞でも区別できない語がある
    * `bank`は銀行、土手の意味がある

英語のstemming

* Poter's stemming
    * 語尾の`ed`を削除
        * `hundred`などは不要に語が削られる
    * 語尾の`ate`を削除
    * 語尾の`ational`を削除

### 2.4.2 日本語の前処理
* 形態素解析(morphological analysis)
    * 日本語のデータは単語で区切られてない
    * 中国個、タイ語もそう
    * 日本語はstemmingはしない
    * `走ら`(ない）、`走り`（たい）、`走る`、`走れ`（ば）などを全て、`走る`に直す、見出し語化が使われる
    * `騒ぎ`は動詞と名詞できる

### 2.4.3
* 国語時点の単語が5万語ほどとすると、新聞記事の単語は数百語
* 一般にスパースになる

## 2.5 単語のベクトル表現
* 単語の文字に意味はないので、文字からベクトル化はあまり有用でない

### 2.5.1 単語トークンの文脈ベクトル表現

 高く **跳ぶ** にはまず屈め

* 文脈ベクトル(context vector)
	* 前後の文脈から作られるベクトル
	* 前のn単語と後のm単語に1をたてたベクトル
	* 上記の例文では、`高く`と`に`の要素に1がたったベクトル
	* 文脈窓(context window)
		* 前のn単語と後のm単語と対称の単語
	* 文脈窓幅(context window size)
		* n + m + 1
		* 定義は人による
	* context windowでの単語の位置を区別してベクトル化することもできる
		* 2単語前にでた単語と1単語前に出た同じ単語を区別してベクトル化	
	* 構文解析を用いる方法もある
		* 動詞と対応する目的語などをベクトルとして含めるなど

### 2.5.2 単語タイプの文脈ベクトルの表現
単語トークンと同じだが、同じ単語は一つにまとめる

## 2.6 文書や単語の確率分布による表現
$$p_{W \mid D}$$で文書$D$が与えられた時の、単語$W$の条件付き確率を与える。
確率の与え方は色々ある。
例えば

$$
	p_{W \mid D}(w \mid d)
	=
	\frac{
		n(d, w)
	}{
		\sum_{\bar{w}} n(d, \bar{w})
	}
$$

とおく。
文書間の距離は、KL-divergenceやSJS-divergenceで測ることができる。


## 2.7 まとめ
* 日本語の形態素解析ツール
	* MeCab
	* Chasen
* 英語の品詞タグ付け
	* TreeTagger	



