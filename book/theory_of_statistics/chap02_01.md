---
title: Definitions
book_title: Theory of statistics
book_chapter: 2
book_section: 1
---

## 2.1 Definitions

### 2.1.1 Notional overviw

* $(S, \mathcal{A}, \mu)$,
    * probability sp.
* $\mathrm{Pr}(A) := \mu(A)$,
* $\mathrm{Pr}(A \mid \cdot) := \mu(A \mid \cdot)$,
* $X: S \rightarrow \mathcal{X}$,
  * random quantity
  * data
* $(\mathcal{X}, \mathcal{B})$,
  * measurable sp.
  * sample sp.
  * usually subset of euclidian space
  * $$\{x\} \in \mathcal{B} \ (x \in \mathcal{X})$$,
  * elments of $\mathcal{X}$ is denoted by $x, y, z, x_{1}, x_{2}, \ldots$
* $\mathcal{A}_{X} := X^{-1}(\mathcal{B})$,
* $(\Omega, \tau)$,
  * measurable sp.
  * $\Omega$ is called parameter sp.
  * $\Omega$ is usually subset of finite-dimensional euclidean space
* $\Theta: S \rightarrow \Omega$,
* $$\mathcal{P}_{0} := \{P_{\theta}\}_{\Omega}$$,
    * parametric family of distributions for $X$,
* $P_{\theta}: \mathcal{B} \rightarrow [0, 1] (\theta \in \Theta)$
    * conditonal distribution of $X$ given $\Theta = \theta$,
* $P_{\theta}^{\prime}: \mathcal{A}_{X} \rightarrow [0, 1] (\theta \in \Theta)$
    * probability measure on $(S, \mathcal{A}_{X})$,

$$
  P_{\theta}(A)
  :=
  \mu(X^{-1}(A) \mid \Theta=\theta)
$$


$$
  P_{\theta}^{\prime}(\cdot)
  :=
  \mathrm{Pr}(\cdot \mid \Theta = \theta)
  .
$$

$$
\begin{eqnarray}
  A \in \mathcal{A}_{X},
  \
  \exists B \in \mathcal{B},
  \
  P_{\theta}^{\prime}(A)
  & = &
    P_{\theta}^{\prime}(X \in B)
  \nonumber
  \\
  & = &
    \mathrm{Pr}(X \in B \mid \Theta = \theta)
  \nonumber
  \\
  & = &
    P_{\theta}(B)
  \nonumber
  .
\end{eqnarray}
$$

### Example 2.1
* $$\{X_{n}\}_{n \in \mathbb{N}}$$,
    * conditonally IID random variable with $\mathrm{N}(\theta, 1)$ given $\Theta = \theta$,
* $X = (X_{1}, \ldots, X_{n})$,

If $n=1$, $B \in \mathcal{B}$ then

$$
\begin{eqnarray}
  P_{\theta}(B)
  & = &
    \int_{B}
      \frac{
        1
      }{
        \sqrt{2 \pi}
      }
      \exp
      \left(
          -
          \frac{
            (x - \theta)^{2}
          }{
            2
          }
      \right)
    \ dx
  \nonumber
  \\
  & = &
    P_{\theta}^{\prime}(X_{i} \in B)
  \nonumber
  \\
  & = &
      \mathrm{Pr}(X_{i} \in B \mid \Theta = \theta)
  \nonumber
\end{eqnarray}
$$

Similarly, $C \in \mathcal{B}^{n}$,

$$
\begin{eqnarray}
  P_{\theta}(C)
  & = &
      \int_{C}
        (2 \pi)^{-n/2}
        \exp
        \left(
            -
            \frac{ 1 }{ 2 }
            \sum_{i=1}^{n}
                (x_{i} - \theta)^{2}
        \right)
      \ dx_{1} \cdots dx_{n}
  \nonumber
  \\
  & = &
    P_{\theta}^{\prime}(X \in C)
  \nonumber
  \\
  & = &
    \mathrm{Pr}(X \in C \mid \Theta = \theta)
    .
\end{eqnarray}
$$

* $\mu_{\Theta}$
    * distribution of $\Theta$,
* $D \in \tau$
* $B \in \mathcal{B}$,
* $A := X^{-1}(B)$,
* $E := \Theta^{-1}(D)$,

We have

$$
\begin{eqnarray}
  \mathrm{Pr}(X \in B, \Theta \in D)
  & = &
    \mu(A \cap E)
  \nonumber
  \\
  & = &
    \int_{E}
      \mathrm{Pr}(A \mid \Theta)(s)
    \ \mu(ds)
  \nonumber
  \\
  & = &
      \int_{D}
        P_{\theta}(B)
      \ \mu_{\Theta}(d \theta)
  \nonumber
\end{eqnarray}
$$

<div class="end-of-statement" style="text-align: right">■</div>

### Example 2.2 
* $\Theta sim \mathrm{N}(0, 1)$,
* $D \in \tau$,
* $B \in \mathcal{B}$,
* $\mathrm{Pr}(\Theta \in D, X \in B)$,

$$
\int_{D}
  \int_{B}
    (2\pi)^{-\frac{n+1}{2}}
    \exp
    \left(
        - \frac{1}{2}
        \left(
            \sum_{j=1}^{n}
                (x_{j} - \theta)^{2}
                +
                \theta^{2}
        \right)
    \right)
  \ dx_{1} \cdots dx_{n}
\ d \theta
.
$$

<div class="end-of-statement" style="text-align: right">■</div>

## 2.1.2 Sufficiency

### Definition 2.3
* $(\mathcal{T}, \mathcal{C})$,
  * measurable sp.
  * $$\forall x \in \mathcal{T}, \{x\} \in \mathcal{C}$$,
* $T: \mathcal{X} \rightarrow \mathcal{T}$,

$T$ is said to be statistc if $T$ is measurable.

<div class="end-of-statement" style="text-align: right">■</div>

The only requirement is that $\mathcal{C}$ must contain singletons.
We denote composite function $T \circ X$ by

$$
  T(X): S \rightarrow \mathcal{T}
  .
$$

$P_{\theta, T}:\mathcal{C} \rightarrow [0, 1] $ is

$$
\begin{eqnarray}
  P_{\theta, T}(C)
  & := &
    P_{\theta}(T^{-1}(C))
  \nonumber
  \\
  & = &
      P_{\theta}^{\prime}(T(X) \in C)
\end{eqnarray}
  .
$$

## Remark
* $\mu_{\Theta \mid X}: \tau \times \mathcal{X} \rightarrow [0, 1]$,
    * onditional probability given $X = x$,

$$
  x \in \mathcal{X},
  \
  B \in \tau,
  \
  \mu_{\Theta \mid X}(B \mid x)
  =
  \mu_{\Theta}(B \mid X = x)
  .
$$

<div class="end-of-statement" style="text-align: right">■</div>


### Definition 2.4
* $\mathcal{P}_{0}$,
  * parametric family of distributions on $(\mathcal{X}, \mathcal{B})$,
* $(\Omega, \tau)$,
* $\Theta: \mathcal{P}_{0} \rightarrow \Omega$,
* $T: \mathcal{X} \rightarrow \mathcal{T}$,
    * statistic
* $\mu_{X}:\mathcal{B} \rightarrow [0, 1]$
    * distribution of $X$
* $\mu_{\Theta}:\mathcal{B} \rightarrow [0, 1]$
    * distribution of $\Theta$

$T$ is a sufficient statistic for $\Theta$ ( in the Bayesian sense) if 

$$
\begin{eqnarray}
  B \in \tau,
  \
  \mu_{\Theta \mid X}(B \mid x)
  & = &
    \mu_{\Theta \mid T}(B \mid T(x))
    \quad
    \mu_{X} \text{-a.s.}
  .
  \nonumber
\end{eqnarray}
$$

<div class="end-of-statement" style="text-align: right">■</div>


### Example 2.5
* $$\{X_{n}\}_{n \in \mathbb{N}}$$,
  * exchangeable Bernoulli random variables
* $$\mathcal{P}_{0} := \{P\}$$,
    * IID distributions
* $X = (X_{1}, \ldots, X_{n})$
* $P_{\theta}$,
  * distribution that says the coordinates of $X$ are IID $\mathrm{Ber}(\theta)$.

Radon-Nikodym derivative

$$
  \frac{
    d \mu_{\Theta \mid X}
  }{
    d \mu_{\Theta}
  }
  (\theta \mid x)
  =
  \frac{
    \theta^{\sum_{i=1}^{n}x_{i}}
     (1 - \theta)^{n - \sum_{i=1}^{n}x_{i}}
  }{
    \int_{}
      \psi^{\sum_{i=1}^{n} x_{i}}
      (1 - \psi)^{n - \sum_{i=1}^{n} x_{i}}
    \ d \mu_{\Theta}(\psi)
  }
$$

Next, treat $T(X) := \sum_{i=1}^{n} X_{i}$ as the data.
The dnesity of $T$ given $\Theta = \theta$ is

$$
  f_{T \mid \Theta}(t \mid \theta)
  =
  \left(
    \begin{array}{c}
      n \\
      t
    \end{array}
  \right)
  \theta^{t}(1 - \theta)^{n-t}
  \
  t = 0, \ldots, n
  .
$$

It follows from Bayes' theorem 1.31 that the posterior given $T = t = \sum_{i=1}^{n} x_{i}$ has derivative

$$
  \frac{
    d \mu_{\Theta \mid T}
  }{
    d \mu_{\Theta}
  }
  =
  \frac{
    \left(
      \begin{array}{c}
        n \\
        t
      \end{array}
    \right)
    \theta^{t}
    (1 - \theta)^{n - t}i
  }{
    \int_{}
      \left(
        \begin{array}{c}
          n \\
          t
        \end{array}
      \right)
      \psi^{t}
      (1 - \psi)^{n - t}
    \ \mu_{\Theta}(d \psi)
  }
  .
$$

Hence $T$ is sufficient.

<div class="end-of-statement" style="text-align: right">■</div>

### Lemma 2.6
* $T: \mathcal{X} \rightarrow \mathcal{T}$
  * statistic
* $\mathcal{B}_{T} := \sigma(T)$,

Then

* (i) $T$ is sufficient in the Bayesian sense
* (ii) there exists a version of the posterior distribution given $X$, $\mu_{\Theta \mid X}$ such that $\forall B \in \tau$, $\mu_{\Theta \mid X}(B \mid \cdot)$ is measurable with respect to $\mathcal{B}_{T}$.

### proof
By applying Theorem B.73.

* $\mathcal{B} = X^{-1}(\mathcal{B})$,
* $\mathcal{C} = X^{-1}(\mathcal{B}_{T})$,
* $Z = 1_{B}(\Theta)$

<div class="QED" style="text-align: right">$\Box$</div>


### Example 2.7
* $\mathcal{P}_{0}$,
  * IID exponential distributions
* $P_{\theta} := \mathrm{Exp}(\theta)$,
* $$\{X_{i}\}_{i \in \mathbb{N}}$$,
* $X = (X_{1}, \ldots, n)$,

$$
  \frac{
    d \mu_{\Theta \mid X}
  }{
    d \mu_{\Theta}
  }(\theta \mid x)
  =
  \frac{
    \theta^{n}
    \exp
    \left(
        - \theta
        \sum_{j=1}^{n}
            x_{j}
    \right)
  }{
    \int_{}
      \psi^{n}
      \exp
      \left(
          - \psi
          \sum_{i=1}^{n}
              x_{i}
      \right)
    \ mu_{\Theta}(d\psi)
  }
$$

<div class="end-of-statement" style="text-align: right">■</div>

### Definition 2.8
* $\mathcal{P}_{0}$,
    * parametric family of distributions on $(\mathcal{X}, \mathcal{B})$,
* $(\Omega, \tau)$,
* $\Theta: \mathcal{P}_{0} \rightarrow \Omega$,
* $T: \mathcal{X} \rightarrow \mathcal{T}$,
  * statistic

Suppose that there eixst versions of $P_{\theta}(\cdot \mid T)$ and a function $r: \mathcal{B} \times \mathcal{T} \rightarrow [0, 1]$ such that

$$
  r(\cdot, t): \mathcal{B} \rightarrow [0, 1]
$$

is a probability for $t \in \mathcal{T}$.
$r(A, \cdot)$ is measurable for $A \in \mathcal{B}$.

$$
  \forall \theta \in \Theta,
  \
  B \in \mathcal{B},
  \
  P_{\theta}(B \mid T = t)
  =
  r(B, t)
  \
  P_{\theta, T} \text{-a.e.}
$$

Then

$T$ is a sufficient static for $\Theta$ in the classical sense.

<div class="end-of-statement" style="text-align: right">■</div>
